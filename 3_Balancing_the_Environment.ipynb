{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Balancing the Environment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrklees/learning-agents/blob/master/3_Balancing_the_Environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KYltSnUAuSN7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A Harder Challenge: A Shifting Environment\n",
        "\n",
        "In this tutorial we will tackle the Cart-Pole task in the Open AI gym.  This task is harder because it adds two new elements.  The state space is continuous and has a temporal element as well.  Therefore our agent must take *observations* from the environment. Second, the reward (or rather lack of punishment) is delayed.  A mistake may lead to failure, but not necessarily in the next frame. "
      ]
    }
  ]
}