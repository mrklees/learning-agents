{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. The 2-armed Bandit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrklees/learning-agents/blob/master/2_The_2_armed_Bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yNvQpMF0Nxfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introducing the Problem\n",
        "\n",
        "In this notebook we [follow along with Arthur](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149) on the famous [multi-armed bandit problem.](https://en.wikipedia.org/wiki/Multi-armed_bandit)  While I frequently dislike working on contrived problems, the multi-armed bandit is a classical problem for the reason that it's very analgous to many decisions we have to make.  The multi-armed bandit problem demonstrates the exploration vs exploitation trade off, and helps us to think about how our agents might tackle such a problem.\n",
        "\n",
        "Something of note.  The basest explanation of our agent's objective is to find the set of actions which maximizes some reward function.  The challenge is that rewards can vary by multiple conditions including:\n",
        "* Rewards can vary by the action you take.\n",
        "* Rewards can be delayed, meaning you might not know immediately if you made a good or bad decision.\n",
        "* Rewards can be conditional on the state of the environment. \n",
        "\n",
        "Multi-armed bandit only suffers from the first problem, as each arm is defined as having a different probability of reward.  However, because we get the rewards immediately and the rewards do not vary over time, the problem doesn't fall into the second two categories.  That makes this a relatively simple case.\n",
        "\n",
        "## Policy Network\n",
        "\n",
        "Our strategy will be to use a policy network which is updated via gradient descent!  The loss function is slightly strange in this case.  It is defined as $$ Loss = -log(\\pi)*A $$ where $A$ is advantage.  Essentially, $A$ corresponds to the reward for some action relative to a baseline.  We'll be assuming in this case that the baseline is 0, and therefore think of $A$ as the reward we recieved for the action (-1 or 1 in this case). $\\pi$ is our policy, which is simply the weight that our network has evaluated for this action.  "
      ]
    },
    {
      "metadata": {
        "id": "eYbYZ46RPEfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3_5Yu27RSjfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our bandits.  The number corresponds to the z-score you have to beat to\n",
        "# recieve a postive reward.  Therefore the last bandit is the easiest to win\n",
        "# on because very nearly all sampled values will be greater than -5.\n",
        "bandits = [0.5, 0, -1, -0.5]\n",
        "num_bandits = len(bandits)\n",
        "def pullBandit(bandit):\n",
        "    # Get a random number\n",
        "    result = np.random.randn(1)\n",
        "    if result > bandit:\n",
        "        # Return a positive reward\n",
        "        return 1.\n",
        "    else:\n",
        "        # Return a negative reward\n",
        "        return -1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OUr_MtuuTILJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Greedy Agent\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# The Policy Network\n",
        "weights = tf.Variable(tf.ones([num_bandits]))\n",
        "## Greedy chosen action = always choose the best one\n",
        "chosen_action = tf.argmax(weights, 0)\n",
        "\n",
        "# The training procedure\n",
        "## Placeholders for the chosen action and reward from the environment\n",
        "reward_ = tf.placeholder(shape=[1], dtype=tf.float32)\n",
        "action_ = tf.placeholder(shape=[1], dtype=tf.int32)\n",
        "## Extract the associate weight so we can update it\n",
        "responsible_weight = weights[action_[0]]\n",
        "## Calculate loss per the loss function above\n",
        "loss = -1*(tf.log(responsible_weight)*reward_)\n",
        "# Optimize!\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dihmbe8iezte",
        "colab_type": "code",
        "outputId": "20ef411b-c6f0-4cd9-fdf5-d0cb2572baa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The way we're defining our agents is a little sloppy, so we are testing it in this kind of ugly way. \n",
        "n_agents = 100\n",
        "total_episodes = 1000 # Set total number of episodes to train agent on.\n",
        "e = 0.1 # Set the chance of taking a random action.\n",
        "\n",
        "print(np.array([train_agent(total_episodes, e=e) for agent in range(n_agents)]).mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ndHaHOCaZqzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Probabilistic Agent\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# The Policy Network\n",
        "weights = tf.Variable(tf.ones([num_bandits]))\n",
        "## The probability of an action chosen is proportional to its weight\n",
        "chosen_action = tf.distributions.Categorical(probs=weights).sample(1)\n",
        "# The training procedure\n",
        "## Placeholders for the chosen action and reward from the environment\n",
        "reward_ = tf.placeholder(shape=[1], dtype=tf.float32)\n",
        "action_ = tf.placeholder(shape=[1], dtype=tf.int32)\n",
        "## Extract the associate weight so we can update it\n",
        "responsible_weight = weights[action_[0]]\n",
        "## Calculate loss per the loss function above\n",
        "loss = -1*(tf.log(responsible_weight)*reward_)\n",
        "# Optimize!\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HoIxhA9ne00J",
        "colab_type": "code",
        "outputId": "e94be224-09d1-4b86-847c-4957a7585e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The way we're defining our agents is a little sloppy, so we are testing it in this kind of ugly way. \n",
        "n_agents = 100\n",
        "total_episodes = 1000 # Set total number of episodes to train agent on.\n",
        "e = 0.1 # Set the chance of taking a random action.\n",
        "\n",
        "print(np.array([train_agent(total_episodes, e=e) for agent in range(n_agents)]).mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mhZcbaEbTJw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train_agent(total_episodes, e = 0.1, verbose=False):\n",
        "\n",
        "    total_reward = np.zeros(num_bandits) # Set scoreboard for bandits to 0.\n",
        "\n",
        "    # Launch the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        for i in range(total_episodes):\n",
        "\n",
        "            #Choose either a random action or one from our network.\n",
        "            if np.random.rand(1) < e:\n",
        "                action = np.random.randint(num_bandits)\n",
        "            else:\n",
        "                action = int(sess.run(chosen_action))\n",
        "\n",
        "            reward = pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n",
        "\n",
        "            #Update the network.\n",
        "            _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_:[reward],action_:[action]})\n",
        "\n",
        "            #Update our running tally of scores.\n",
        "            total_reward[action] += reward\n",
        "            if (i % 50 == 0) & verbose:\n",
        "                print( \"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
        "    if verbose: print( \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
        "    if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
        "        if verbose: print(\"...and it was right!\")\n",
        "        return 1\n",
        "    else:\n",
        "        if verbose: print(\"...and it was wrong!\")\n",
        "        return 0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "du81cmOjZpBH",
        "colab_type": "code",
        "outputId": "18139ca0-e838-4b9a-ef02-a4380a3ceac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "n_agents = 10\n",
        "total_episodes = 1000 # Set total number of episodes to train agent on.\n",
        "e = 0.1 # Set the chance of taking a random action.\n",
        "\n",
        "print(np.array([train_agent(total_episodes, e=e) for agent in range(n_agents)]).mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mb_Gt2C0Ynrf",
        "colab_type": "code",
        "outputId": "20ba6e1d-8ca3-4520-fccf-ba17ab2d9e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    print(sess.run(tf.nn.softmax([-1., 0., 0., 0.])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.10923178 0.29692274 0.29692274 0.29692274]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vA4WKFYjVQTK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Our agent is... okay\n",
        "\n",
        "When a bandit was much better than the others, such as when one had a threshold of -5, our agent very easily solved the problem within the fixed number of episodes.  However, when we altered the payoffs so that they were closer together, our agent didn't not fair so well.  In one observed example, it got fixed on exploiting the bandit that was closest to correct and rarely even explored the better option. \n",
        "\n",
        "We won't actually address this issue, as doing so requires slightly more sophisticated logic.  For example, instead of the greedy approach we probably want one that exploits likely beneficial path even if they aren't the best known path.  A simple way to do this would be to make the probability of choosing some action proportational to the cumulative reward recorded for that action. \n",
        "\n",
        "**Update:**  After adding a second agent which did exactly that, we saw a significant improvement.  Whereas the greedy agent was only correct 79% of the time, our probabilistic agent actually was correct 100% of the time in 100 tries. \n",
        "\n",
        "# Contextual Bandits\n",
        "\n",
        "In this variation of Multi-arm Bandit, we generalize the first problem by introducing state.  Now, instead of just optimizing our decision for a single row of slot machines (or however you like to visualize the problem), we are optimizing our decision making for each of a set of n rows of slot machines.\n",
        "\n",
        "This becomes more concrete with code."
      ]
    },
    {
      "metadata": {
        "id": "kjykfWL4gZZF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class contextual_bandit():\n",
        "    def __init__(self):\n",
        "        # The state tracks which row we are in\n",
        "        self.state = 0\n",
        "        # List our bandits as a 2 dimensional array where we are trying to \n",
        "        # optimize decision making for each row of bandits.\n",
        "        self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5]])\n",
        "        \n",
        "        self.num_rows = self.bandits.shape[0]\n",
        "        self.num_bandits = self.bandits.shape[1]\n",
        "        \n",
        "    def getRow(self):\n",
        "        # Randomly determine which row we are in each episode\n",
        "        self.state = np.random.randint(0, self.num_rows)\n",
        "        return self.state\n",
        "    \n",
        "    def pullArm(self, action):\n",
        "        # Get a random number\n",
        "        bandit = self.bandits[self.state, action]\n",
        "        result = np.random.randn(1)\n",
        "        if result > bandit:\n",
        "            #return a positive reward\n",
        "            return 1\n",
        "        else:\n",
        "            return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKxeapTui_1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class agent():\n",
        "    def __init__(self, lr, s_size, a_size, greedy=True):\n",
        "        # Defines the Feedforward Network\n",
        "        \n",
        "        with tf.variable_scope('network'):\n",
        "            # Take the current state as an integer for an input\n",
        "            self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
        "            # One hot encode for our network\n",
        "            state_in_OH = tf.one_hot(self.state_in, s_size)\n",
        "            # Pass the state into a Dense layer\n",
        "            output =  tf.contrib.layers.fully_connected(inputs=state_in_OH, \n",
        "                                     num_outputs=a_size, biases_initializer=None,\n",
        "                                     activation_fn=tf.nn.sigmoid, \n",
        "                                     weights_initializer=tf.ones_initializer())\n",
        "            self.output = tf.reshape(output, [-1])\n",
        "            if greedy:\n",
        "                self.chosen_action = tf.argmax(self.output, 0)\n",
        "            else:\n",
        "                self.chosen_action = tf.distributions.Categorical(probs=self.output).sample(1)[0]\n",
        "            \n",
        "        with tf.variable_scope('training'):\n",
        "            # Feed in reward and chosen action and then compute the loss\n",
        "            # This is identical to our prior agent. \n",
        "            self.reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "            self.action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "            self.responsible_weight = self.output[self.action_holder[0]]\n",
        "            self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "            self.update = optimizer.minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HmmVlZkSjEOD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_agent(episodes=1000, greedy=True, verbose=False):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    cBandit = contextual_bandit()\n",
        "    sAgent = agent(lr=0.001, s_size=cBandit.num_rows, a_size=cBandit.num_bandits, greedy=False)\n",
        "    weights = tf.trainable_variables()[0]\n",
        "\n",
        "    total_episodes = 10000\n",
        "\n",
        "    total_reward = np.zeros([cBandit.num_rows, cBandit.num_bandits])\n",
        "    e = 0.1\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "\n",
        "        for i in range(total_episodes):\n",
        "            s = cBandit.getRow() # Get state from the environment\n",
        "\n",
        "            # Sometimes do something random\n",
        "            if np.random.rand(1) < e:\n",
        "                action = np.random.randint(cBandit.num_bandits)\n",
        "            else:\n",
        "                action = sess.run(sAgent.chosen_action, feed_dict={sAgent.state_in:[s]})\n",
        "\n",
        "            # Get Reward\n",
        "            reward = cBandit.pullArm(action)\n",
        "\n",
        "            # Update the network\n",
        "            feed={sAgent.reward_holder:[reward],\n",
        "                       sAgent.action_holder:[action],\n",
        "                       sAgent.state_in:[s]}\n",
        "            _, ww = sess.run([sAgent.update, weights], feed_dict=feed)\n",
        "\n",
        "            # Update the running tally of scores\n",
        "            total_reward[s, action] += reward\n",
        "            if i % 500 == 0:\n",
        "                if verbose: print(\"Mean reward for each of the \" + str(cBandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1)))\n",
        "    victories = 0\n",
        "    for a in range(cBandit.num_rows):\n",
        "        if verbose: print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n",
        "        if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
        "            if verbose: print(\"...and it was right!\")\n",
        "            victories += 1\n",
        "        else:\n",
        "            if verbose: print(\"...and it was wrong!\")\n",
        "    return victories/cBandit.num_rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8uFPaOVukdNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68b577b7-92be-4b09-f5a0-09807a030170"
      },
      "cell_type": "code",
      "source": [
        "n_agents = 50\n",
        "# Compare the accuracy of greedy vs non greedy agents. \n",
        "print(f\"The Greedy Agent's score: {np.array([test_agent(greedy=True, verbose=False) for agent in range(n_agents)]).mean()}\")\n",
        "print(f\"The Greedy Agent's score: {np.array([test_agent(greedy=False, verbose=False) for agent in range(n_agents)]).mean()}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Greedy Agent's score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LVCaveYKnaRb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}